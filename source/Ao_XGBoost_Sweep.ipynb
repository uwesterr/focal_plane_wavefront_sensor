{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l75qc983sEh8"
      },
      "source": [
        "\n",
        "## Sweeps: An Overview\n",
        "\n",
        "Running a hyperparameter sweep with Weights & Biases is very easy. There are just 3 simple steps:\n",
        "\n",
        "1. **Define the sweep:** we do this by creating a dictionary-like object that specifies the sweep: which parameters to search through, which search strategy to use, which metric to optimize.\n",
        "\n",
        "2. **Initialize the sweep:** with one line of code we initialize the sweep and pass in the dictionary of sweep configurations:\n",
        "`sweep_id = wandb.sweep(sweep_config)`\n",
        "\n",
        "3. **Run the sweep agent:** also accomplished with one line of code, we call w`andb.agent()` and pass the `sweep_id` along with a function that defines your model architecture and trains it:\n",
        "`wandb.agent(sweep_id, function=train)`\n",
        "\n",
        "And voila! That's all there is to running a hyperparameter sweep!\n",
        "\n",
        "In the notebook below, we'll walk through these 3 steps in more detail.\n",
        "\n",
        "We highly encourage you to fork this notebook, tweak the parameters, or try the model with your own dataset!\n",
        "\n",
        "### Resources\n",
        "- [Sweeps docs →](https://docs.wandb.com/library/sweeps)\n",
        "- [Launching from the command line →](https://www.wandb.com/articles/hyperparameter-tuning-as-easy-as-1-2-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SiborLNTviW"
      },
      "outputs": [],
      "source": [
        "#!pip install wandb\n",
        "import os\n",
        "import wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8NZSgFqiJz9"
      },
      "source": [
        "## 1. Define the Sweep\n",
        "\n",
        "Weights & Biases sweeps give you powerful levers to configure your sweeps exactly how you want them, with just a few lines of code. The sweeps config can be defined as\n",
        "[a dictionary or a YAML file](https://docs.wandb.ai/guides/sweeps/configuration).\n",
        "\n",
        "Let's walk through some of them together:\n",
        "*   **Metric** – This is the metric the sweeps are attempting to optimize. Metrics can take a `name` (this metric should be logged by your training script) and a `goal` (`maximize` or `minimize`). \n",
        "*   **Search Strategy** – Specified using the `\"method\"` key. We support several different search strategies with sweeps. \n",
        "  *   **Grid Search** – Iterates over every combination of hyperparameter values.\n",
        "  *   **Random Search** – Iterates over randomly chosen combinations of hyperparameter values.\n",
        "  *   **Bayesian Search** – Creates a probabilistic model that maps hyperparameters to probability of a metric score, and chooses parameters with high probability of improving the metric. The objective of Bayesian optimization is to spend more time in picking the hyperparameter values, but in doing so trying out fewer hyperparameter values.\n",
        "*   **Parameters** – A dictionary containing the hyperparameter names, and discrete values, a range, or distributions from which to pull their values on each iteration.\n",
        "\n",
        "You can find a list of all configuration options [here](https://docs.wandb.com/library/sweeps/configuration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHw_OmGFUC4G"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Ao_XGBoost_Sweep.ipynb\"\n",
        "\n",
        "sweep_config = {\n",
        "    \"method\": \"random\", # try grid or random\n",
        "    \"metric\": {\n",
        "      \"name\": \"error\",\n",
        "      \"goal\": \"minimize\"   \n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"booster\": {\n",
        "            \"values\": [\"gbtree\"]\n",
        "        },\n",
        "        \"max_depth\": {\n",
        "      \"min\": 6,\n",
        "      \"max\": 10\n",
        "        },\n",
        "#    \"learning_rate\" :{\n",
        " #     \"distribution\": \"log_uniform\",\n",
        " #     \"min\": -4,\n",
        " #     \"max\": -2\n",
        " #       },\n",
        "    \"learning_rate\" :{\n",
        "        \"values\": [0.04954]\n",
        "        },        \n",
        "    \"subsample\": {\n",
        "        \"values\": [0.4372]\n",
        "    },\n",
        "    \"colsample_bynode\": {\n",
        "        \"values\": [0.7597]\n",
        "    },    \n",
        "    \"n_estimators\": {\n",
        "        \"values\": [3000, 10000, 20000]\n",
        "    },\n",
        "    \"reg_alpha\": {\n",
        "        \"values\": [0.1668]\n",
        "    },    \n",
        "    \"reg_lambda\": {\n",
        "        \"values\": [0.5222]\n",
        "    }                \n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQc9kV1PiTLi"
      },
      "source": [
        "## 2. Initialize the Sweep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbRIMsYvuTEc"
      },
      "source": [
        "Calling `wandb.sweep` starts a Sweep Controller --\n",
        "a centralized process that provides settings of the `parameters` to any who query it\n",
        "and expects them to return performance on `metrics` via `wandb` logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ7BdPM3VVKx"
      },
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"XGBoost-sweeps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww4_-CDYiZ8f"
      },
      "source": [
        "### Define your training process\n",
        "Before we can run the sweep,\n",
        "we need to define a function that creates and trains the model --\n",
        "the function that takes in hyperparameter values and spits out metrics.\n",
        "\n",
        "We'll also need `wandb` to be integrated into our script.\n",
        "There's three main components:\n",
        "*   `wandb.init()` – Initialize a new W&B run. Each run is single execution of the training script.\n",
        "*   `wandb.config` – Save all your hyperparameters in a config object. This lets you use [our app](https://wandb.ai) to sort and compare your runs by hyperparameter values.\n",
        "*   `wandb.log()` – Logs metrics and custom objects – these can be images, videos, audio files, HTML, plots, point clouds etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBEHFpQ1vi2B"
      },
      "source": [
        "We also need to download the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Lqp6cOFR2cS"
      },
      "outputs": [],
      "source": [
        "# XGBoost model for Pima Indians dataset\n",
        "from numpy import loadtxt\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pyarrow.feather as feather\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import numpy as np\n",
        "import math\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fluxData_df = feather.read_feather('data/fluxData.feather')\n",
        "zernikeData_df = feather.read_feather('data/zernikeData.feather')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def train():\n",
        "  config_defaults = {\n",
        "    \"booster\": \"gbtree\",\n",
        "    \"max_depth\": 3,\n",
        "    \"learning_rate\": 0.1,\n",
        "    \"subsample\": 1,\n",
        "    \"seed\": 117,\n",
        "    \"test_size\": 0.33,\n",
        "  }\n",
        "\n",
        "  wandb.init(config=config_defaults)  # defaults are over-ridden during the sweep\n",
        "  config = wandb.config\n",
        "\n",
        "  # load data and split into predictors and targets\n",
        "  X, Y = fluxData_df, zernikeData_df\n",
        "\n",
        "  # split data into train and test sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
        "                                                      test_size=config.test_size,\n",
        "                                                      random_state=config.seed)\n",
        "\n",
        "  # fit model on train\n",
        "# fitting\n",
        "  result = MultiOutputRegressor(xgb.XGBRegressor(objective='reg:squarederror', colsample_bynode = config.colsample_bynode, max_depth = config.max_depth, reg_alpha=config.reg_alpha, reg_lambda = config.reg_lambda, subsample = config.subsample,\n",
        "  n_estimators = config.n_estimators, learning_rate = config.learning_rate, tree_method =\"hist\", booster =config.booster)).fit(X_train, y_train)\n",
        "  predictResult = result.predict(X_test)\n",
        "  predictError =predictResult-y_test\n",
        "  tmp = predictError.to_numpy().flatten()\n",
        "  error=np.sqrt(np.sum(tmp**2)/tmp.size)\n",
        "  wandb.log({\"error\": error})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmmVE5YTilK-"
      },
      "source": [
        "## 3. Run the Sweep with an agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca81aOiLxen3"
      },
      "source": [
        "Now, we call `wandb.agent` to start up our sweep.\n",
        "\n",
        "You can call `wandb.agent` on any machine where you're logged into W&B that has\n",
        "- the `sweep_id`,\n",
        "- the dataset and `train` function\n",
        "\n",
        "and that machine will join the sweep!\n",
        "\n",
        "> _Note_: a `random` sweep will by defauly run forever,\n",
        "trying new parameter combinations until the cows come home --\n",
        "or until you [turn the sweep off from the app UI](https://docs.wandb.ai/ref/app/features/sweeps).\n",
        "You can prevent this by providing the total `count` of runs you'd like the `agent` to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "coDcjoF_SlMP"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, train, count=2500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": " Using W&B Sweeps with XGBoost",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "643de512da3ef9692daa896915c9c6edbec44805ccf75c0ed51aafb5eb9790df"
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}