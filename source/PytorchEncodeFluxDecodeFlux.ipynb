{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Encode flux and decode flux\n",
    "The idea is to encode flux to a low dimension of about 4, use the econder to create extra input to a neural net by combining flux data and predictions of the encoder\n",
    "runs with 3.8.2 on linux and 3.8.8 on mac"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build data loader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pyarrow.feather as feather\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def build_dataset(batchSizeTrain, batchsizeValid):\n",
    "    fluxData_df = feather.read_feather('data/fluxData.feather')\n",
    "    # normalize input data\n",
    "    fluxData_df_norm = (fluxData_df - fluxData_df.mean(axis=0)) / fluxData_df.std(axis=0)\n",
    "    zernikeData_df = feather.read_feather('data/zernikeData.feather')\n",
    "    X_train, X_val, y_train, y_val = train_test_split(fluxData_df_norm, fluxData_df_norm, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    train_target = torch.tensor(y_train.values.astype(np.float32))\n",
    "    trainInput = torch.tensor(X_train.values.astype(np.float32))\n",
    "\n",
    "    train_tensor = torch.utils.data.TensorDataset(trainInput, train_target) \n",
    "    loaderTrain = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batchSizeTrain, shuffle = True)\n",
    "\n",
    "\n",
    "    valid_target = torch.tensor(y_val.values.astype(np.float32))\n",
    "    validInput = torch.tensor(X_val.values.astype(np.float32))\n",
    "\n",
    "    train_tensor = torch.utils.data.TensorDataset(validInput, valid_target) \n",
    "    loaderValid = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batchsizeValid, shuffle = True)\n",
    "    return loaderTrain, loaderValid\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build data loader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build AO network as class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder1 = nn.Linear(19,config.endcLayer1)\n",
    "        self.encoder2 = nn.Linear(config.endcLayer1,config.endcLayer2)\n",
    "        self.encoderOut = nn.Linear(config.endcLayer2, config.endcLayer3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, act):\n",
    "        x = self.encoder1(x)\n",
    "        x = nn.ReLU(x)\n",
    "        x = self.encoder2(x)\n",
    "        x = nn.ReLU(x)\n",
    "        x = self.encoderOut(x)\n",
    "        return(x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.decoder1 = nn.Linear(config.endcLayer3,config.decodLayer1)\n",
    "        self.decoder1 = nn.Linear(config.decodLayer1,config.decodLayer2)\n",
    "        self.decoderOut = nn.Linear(config.decodLayer2,config.endcLayer3)\n",
    "\n",
    "\n",
    "    def forward(self, x, act):\n",
    "        x = self.decoder1(x)\n",
    "        x = nn.ReLU(x)\n",
    "        x = self.decoder2(x)\n",
    "        x = nn.ReLU(x)\n",
    "        x = self.decoderOut(x)\n",
    "        return(x)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(19,config.endcLayer1),\n",
    "            nn.Linear(config.endcLayer1,config.endcLayer2),\n",
    "            nn.Linear(config.endcLayer2, config.endcLayer3)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(config.endcLayer3,config.decodLayer1),\n",
    "            nn.Linear(config.decodLayer1,config.decodLayer2),\n",
    "            nn.Linear(config.decodLayer2,config.out)\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x,):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import wandb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "config = dict(\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    lrFactor=0.5,\n",
    "    endcLayer1=100,\n",
    "    endcLayer2=50,    \n",
    "    endcLayer3=4,    \n",
    "    decodLayer1=50,    \n",
    "    decodLayer2=100,    \n",
    "    out=19\n",
    "  \n",
    "\n",
    "\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def train(config=None):\n",
    "    print(\"next is init cmd\")\n",
    "\n",
    "    with wandb.init(project=\"EncodeFluxDecodeFlux\", config=config):\n",
    "        print(\"las was init cmd\")\n",
    "       # print(Encoder())\n",
    "\n",
    "        config = wandb.config\n",
    "        model = Autoencoder(config).to(device) \n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=config.lrFactor, cooldown=5, patience=5,\n",
    "        verbose=True)        \n",
    "        print(\"next ist watch command\")\n",
    "        wandb.watch(model, log_freq=10)\n",
    "        print(\"watch cmd finished\")\n",
    "        loaderTrain, loaderValid =build_dataset(1024,1024)\n",
    "        for epoch in range(config.epochs):\n",
    "            model.train()\n",
    "            for i, data in enumerate(loaderTrain,0):\n",
    "                input, labels = data\n",
    "                input = input.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = nn.MSELoss()\n",
    "                model(input)\n",
    "                loss =loss(model(input), labels)\n",
    "        # ⬅ Backward pass + weight update\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            for i, data in enumerate(loaderValid,0):\n",
    "                input, labels = data\n",
    "                input = input.to(device)\n",
    "                labels = labels.to(device)            \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    lossVal = nn.MSELoss()            \n",
    "                    lossVal =lossVal(model(input), labels)\n",
    "                    RmsLossValid=torch.sqrt(lossVal) \n",
    "            \n",
    "            scheduler.step(RmsLossValid.item())  \n",
    "            actualLR = optimizer.param_groups[0][\"lr\"]          \n",
    "            print(\"epoch: \", epoch, \"loss: \", \"%.6f\" % loss.item() ,\"RmsLossValid: \" , \"%.6f\" % RmsLossValid.item(), \"Learning rate:\", \"%.8f\" % actualLR)  \n",
    "\n",
    "            wandb.log({\"loss\": loss, \"RmsLossValid\": RmsLossValid, \"LearningRate\": actualLR})\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            \"RmsLossValid\": RmsLossValid\n",
    "            }, \"PaperModel.pt\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train(config)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "next is init cmd\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muwe-sterr\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">silver-gorge-20</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/uwe-sterr/EncodeFluxDecodeFlux\" target=\"_blank\">https://wandb.ai/uwe-sterr/EncodeFluxDecodeFlux</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/uwe-sterr/EncodeFluxDecodeFlux/runs/1n9qg6d4\" target=\"_blank\">https://wandb.ai/uwe-sterr/EncodeFluxDecodeFlux/runs/1n9qg6d4</a><br/>\n",
       "                Run data is saved locally in <code>/Users/uwesterr/CloudProjectsUnderWork/ProjectsUnderWork/SydneyAO/focal_plane_wavefront_sensor/source/wandb/run-20210726_113857-1n9qg6d4</code><br/><br/>\n",
       "            "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "las was init cmd\n",
      "next ist watch command\n",
      "watch cmd finished\n",
      "epoch:  0 loss:  0.372663 RmsLossValid:  0.612577 Learning rate: 0.00100000\n",
      "epoch:  1 loss:  0.325009 RmsLossValid:  0.560718 Learning rate: 0.00100000\n",
      "epoch:  2 loss:  0.324416 RmsLossValid:  0.554673 Learning rate: 0.00100000\n",
      "epoch:  3 loss:  0.311529 RmsLossValid:  0.567765 Learning rate: 0.00100000\n",
      "epoch:  4 loss:  0.321443 RmsLossValid:  0.566592 Learning rate: 0.00100000\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 43386<br/>Program ended successfully."
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a86f9017b6246e7a3675a9b666345fc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find user logs for this run at: <code>/Users/uwesterr/CloudProjectsUnderWork/ProjectsUnderWork/SydneyAO/focal_plane_wavefront_sensor/source/wandb/run-20210726_113857-1n9qg6d4/logs/debug.log</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find internal logs for this run at: <code>/Users/uwesterr/CloudProjectsUnderWork/ProjectsUnderWork/SydneyAO/focal_plane_wavefront_sensor/source/wandb/run-20210726_113857-1n9qg6d4/logs/debug-internal.log</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>0.32144</td></tr><tr><td>RmsLossValid</td><td>0.56659</td></tr><tr><td>LearningRate</td><td>0.001</td></tr><tr><td>_runtime</td><td>10</td></tr><tr><td>_timestamp</td><td>1627292347</td></tr><tr><td>_step</td><td>4</td></tr></table>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>loss</td><td>█▃▂▁▂</td></tr><tr><td>RmsLossValid</td><td>█▂▁▃▂</td></tr><tr><td>LearningRate</td><td>▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▅▅█</td></tr><tr><td>_timestamp</td><td>▁▁▅▅█</td></tr><tr><td>_step</td><td>▁▃▅▆█</td></tr></table><br/>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">silver-gorge-20</strong>: <a href=\"https://wandb.ai/uwe-sterr/EncodeFluxDecodeFlux/runs/1n9qg6d4\" target=\"_blank\">https://wandb.ai/uwe-sterr/EncodeFluxDecodeFlux/runs/1n9qg6d4</a><br/>\n",
       "                "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "7240fa07bd5f0492db6e5998fcde1467c49f289639e3a06c91ae7c487c9ff707"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}