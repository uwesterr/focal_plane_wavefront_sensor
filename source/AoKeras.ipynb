{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import pyarrow.feather as feather\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "#from tensorflow.keras.layers.experimental import preprocessing\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# W & B test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "# !wandb login\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# fluxData_df = feather.read_feather('data/fluxData.feather')\n",
    "# # zernikeData_df = feather.read_feather('data/zernikeData.feather')\n",
    "# fluxData_df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## import data as h5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "fluxData_df =pd.read_hdf(\"data/fluxData_df.h5\", key=\"fluxData_df\")\n",
    "\n",
    "zernikeData_df =pd.read_hdf(\"data/zernikeData_df.h5\", key=\"zernikeData_df\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Prepare the training dataset\n",
    "x_train = fluxData_df\n",
    "y_train = zernikeData_df\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.1, random_state=42)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Box-Cox transform\n",
    "turned out to be of no effect"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "pt = PowerTransformer()\n",
    "\n",
    "#x_trainTransform = pt.fit_transform(x_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalize data\n",
    "as shown in https://keras.io/guides/preprocessing_layers/#normalizing-numerical-features "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\n",
    "x_train_norm = (x_train - x_train.mean(axis=0)) / x_train.std(axis=0)\n",
    "x_train_norm"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       FluxCoeef1  FluxCoeef2  FluxCoeef3  FluxCoeef4  FluxCoeef5  FluxCoeef6  \\\n",
       "0        0.900396   -0.312539   -0.772603   -0.136057   -0.515690    1.002374   \n",
       "1       -0.006083    0.690461    0.706083    0.001087   -0.135921   -0.396939   \n",
       "2        0.181662   -1.537365    0.046234   -0.595047    0.581989   -0.771069   \n",
       "3        1.104926   -0.493233    1.156159    1.508609   -0.510300   -0.986929   \n",
       "4       -2.766798    0.836719   -1.246774   -1.907403   -2.494540    2.514527   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "58735    1.111759   -1.962699    1.064218    0.667639   -1.961451   -1.014588   \n",
       "58736   -0.892319   -0.834533   -1.203572   -1.178698    0.789761    0.682359   \n",
       "58737    0.783504   -1.503598   -0.629578    1.116959    1.017882    0.327938   \n",
       "58738   -1.123925    2.829232    0.281349    0.459110   -0.539199    0.814040   \n",
       "58739   -0.419711    0.062863    1.067361   -0.669245   -0.445627    0.173409   \n",
       "\n",
       "       FluxCoeef7  FluxCoeef8  FluxCoeef9  FluxCoeef10  FluxCoeef11  \\\n",
       "0       -1.519562   -1.267677    2.596329    -1.010410    -0.143842   \n",
       "1       -0.426242    0.680082   -1.163970     0.370308    -0.359454   \n",
       "2        0.281238    0.494130   -1.038512    -0.823826     2.898185   \n",
       "3       -1.012457    0.279925    0.245774     0.918173    -0.830261   \n",
       "4        1.213441   -0.793285   -1.033874    -0.097034     0.795638   \n",
       "...           ...         ...         ...          ...          ...   \n",
       "58735    0.223547   -1.405704   -0.003764    -0.830787    -0.682253   \n",
       "58736    2.491787   -0.012967   -1.165161    -0.431385     0.405629   \n",
       "58737    1.402795   -0.098698   -0.405518     0.629009    -1.205222   \n",
       "58738   -0.162167    0.123279   -0.639389     1.866671    -1.661150   \n",
       "58739    1.245322    1.689096   -1.082880     1.649623    -0.570011   \n",
       "\n",
       "       FluxCoeef12  FluxCoeef13  FluxCoeef14  FluxCoeef15  FluxCoeef16  \\\n",
       "0        -2.373536     1.105245     2.871782    -0.608565    -0.844337   \n",
       "1        -0.103394    -0.670624     0.551241    -0.832305    -0.111359   \n",
       "2         1.414300    -0.651543    -1.063024     0.513900     1.057111   \n",
       "3        -0.717984    -0.352607     0.253269     0.497768     0.852767   \n",
       "4         1.403599     0.435049    -0.978962     2.012168    -0.004434   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "58735    -0.615273     0.168018    -0.899590     0.452977     1.137328   \n",
       "58736    -0.740750    -0.911497     0.110287    -0.775899     2.467752   \n",
       "58737    -0.989608    -1.052450    -1.082164    -0.551245     2.690250   \n",
       "58738    -1.217272    -0.752796     0.835984    -0.637278    -0.090449   \n",
       "58739    -0.560612    -1.209715     0.385461    -0.904895     0.965109   \n",
       "\n",
       "       FluxCoeef17  FluxCoeef18  FluxCoeef19  \n",
       "0         0.683551     1.348749    -1.338257  \n",
       "1        -0.335083     0.090348    -0.070232  \n",
       "2        -0.846276    -0.285620     0.325633  \n",
       "3         0.561766    -0.343766    -0.816310  \n",
       "4         0.344632     0.770427     1.462520  \n",
       "...            ...          ...          ...  \n",
       "58735     0.070017     3.806415     0.336357  \n",
       "58736     0.814036    -0.972636     1.145398  \n",
       "58737    -0.919279    -1.418809    -0.327146  \n",
       "58738     2.143275     1.077346    -0.202596  \n",
       "58739     0.192485    -0.500860     0.058454  \n",
       "\n",
       "[58740 rows x 19 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FluxCoeef1</th>\n",
       "      <th>FluxCoeef2</th>\n",
       "      <th>FluxCoeef3</th>\n",
       "      <th>FluxCoeef4</th>\n",
       "      <th>FluxCoeef5</th>\n",
       "      <th>FluxCoeef6</th>\n",
       "      <th>FluxCoeef7</th>\n",
       "      <th>FluxCoeef8</th>\n",
       "      <th>FluxCoeef9</th>\n",
       "      <th>FluxCoeef10</th>\n",
       "      <th>FluxCoeef11</th>\n",
       "      <th>FluxCoeef12</th>\n",
       "      <th>FluxCoeef13</th>\n",
       "      <th>FluxCoeef14</th>\n",
       "      <th>FluxCoeef15</th>\n",
       "      <th>FluxCoeef16</th>\n",
       "      <th>FluxCoeef17</th>\n",
       "      <th>FluxCoeef18</th>\n",
       "      <th>FluxCoeef19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900396</td>\n",
       "      <td>-0.312539</td>\n",
       "      <td>-0.772603</td>\n",
       "      <td>-0.136057</td>\n",
       "      <td>-0.515690</td>\n",
       "      <td>1.002374</td>\n",
       "      <td>-1.519562</td>\n",
       "      <td>-1.267677</td>\n",
       "      <td>2.596329</td>\n",
       "      <td>-1.010410</td>\n",
       "      <td>-0.143842</td>\n",
       "      <td>-2.373536</td>\n",
       "      <td>1.105245</td>\n",
       "      <td>2.871782</td>\n",
       "      <td>-0.608565</td>\n",
       "      <td>-0.844337</td>\n",
       "      <td>0.683551</td>\n",
       "      <td>1.348749</td>\n",
       "      <td>-1.338257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.006083</td>\n",
       "      <td>0.690461</td>\n",
       "      <td>0.706083</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>-0.135921</td>\n",
       "      <td>-0.396939</td>\n",
       "      <td>-0.426242</td>\n",
       "      <td>0.680082</td>\n",
       "      <td>-1.163970</td>\n",
       "      <td>0.370308</td>\n",
       "      <td>-0.359454</td>\n",
       "      <td>-0.103394</td>\n",
       "      <td>-0.670624</td>\n",
       "      <td>0.551241</td>\n",
       "      <td>-0.832305</td>\n",
       "      <td>-0.111359</td>\n",
       "      <td>-0.335083</td>\n",
       "      <td>0.090348</td>\n",
       "      <td>-0.070232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.181662</td>\n",
       "      <td>-1.537365</td>\n",
       "      <td>0.046234</td>\n",
       "      <td>-0.595047</td>\n",
       "      <td>0.581989</td>\n",
       "      <td>-0.771069</td>\n",
       "      <td>0.281238</td>\n",
       "      <td>0.494130</td>\n",
       "      <td>-1.038512</td>\n",
       "      <td>-0.823826</td>\n",
       "      <td>2.898185</td>\n",
       "      <td>1.414300</td>\n",
       "      <td>-0.651543</td>\n",
       "      <td>-1.063024</td>\n",
       "      <td>0.513900</td>\n",
       "      <td>1.057111</td>\n",
       "      <td>-0.846276</td>\n",
       "      <td>-0.285620</td>\n",
       "      <td>0.325633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.104926</td>\n",
       "      <td>-0.493233</td>\n",
       "      <td>1.156159</td>\n",
       "      <td>1.508609</td>\n",
       "      <td>-0.510300</td>\n",
       "      <td>-0.986929</td>\n",
       "      <td>-1.012457</td>\n",
       "      <td>0.279925</td>\n",
       "      <td>0.245774</td>\n",
       "      <td>0.918173</td>\n",
       "      <td>-0.830261</td>\n",
       "      <td>-0.717984</td>\n",
       "      <td>-0.352607</td>\n",
       "      <td>0.253269</td>\n",
       "      <td>0.497768</td>\n",
       "      <td>0.852767</td>\n",
       "      <td>0.561766</td>\n",
       "      <td>-0.343766</td>\n",
       "      <td>-0.816310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.766798</td>\n",
       "      <td>0.836719</td>\n",
       "      <td>-1.246774</td>\n",
       "      <td>-1.907403</td>\n",
       "      <td>-2.494540</td>\n",
       "      <td>2.514527</td>\n",
       "      <td>1.213441</td>\n",
       "      <td>-0.793285</td>\n",
       "      <td>-1.033874</td>\n",
       "      <td>-0.097034</td>\n",
       "      <td>0.795638</td>\n",
       "      <td>1.403599</td>\n",
       "      <td>0.435049</td>\n",
       "      <td>-0.978962</td>\n",
       "      <td>2.012168</td>\n",
       "      <td>-0.004434</td>\n",
       "      <td>0.344632</td>\n",
       "      <td>0.770427</td>\n",
       "      <td>1.462520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58735</th>\n",
       "      <td>1.111759</td>\n",
       "      <td>-1.962699</td>\n",
       "      <td>1.064218</td>\n",
       "      <td>0.667639</td>\n",
       "      <td>-1.961451</td>\n",
       "      <td>-1.014588</td>\n",
       "      <td>0.223547</td>\n",
       "      <td>-1.405704</td>\n",
       "      <td>-0.003764</td>\n",
       "      <td>-0.830787</td>\n",
       "      <td>-0.682253</td>\n",
       "      <td>-0.615273</td>\n",
       "      <td>0.168018</td>\n",
       "      <td>-0.899590</td>\n",
       "      <td>0.452977</td>\n",
       "      <td>1.137328</td>\n",
       "      <td>0.070017</td>\n",
       "      <td>3.806415</td>\n",
       "      <td>0.336357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58736</th>\n",
       "      <td>-0.892319</td>\n",
       "      <td>-0.834533</td>\n",
       "      <td>-1.203572</td>\n",
       "      <td>-1.178698</td>\n",
       "      <td>0.789761</td>\n",
       "      <td>0.682359</td>\n",
       "      <td>2.491787</td>\n",
       "      <td>-0.012967</td>\n",
       "      <td>-1.165161</td>\n",
       "      <td>-0.431385</td>\n",
       "      <td>0.405629</td>\n",
       "      <td>-0.740750</td>\n",
       "      <td>-0.911497</td>\n",
       "      <td>0.110287</td>\n",
       "      <td>-0.775899</td>\n",
       "      <td>2.467752</td>\n",
       "      <td>0.814036</td>\n",
       "      <td>-0.972636</td>\n",
       "      <td>1.145398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58737</th>\n",
       "      <td>0.783504</td>\n",
       "      <td>-1.503598</td>\n",
       "      <td>-0.629578</td>\n",
       "      <td>1.116959</td>\n",
       "      <td>1.017882</td>\n",
       "      <td>0.327938</td>\n",
       "      <td>1.402795</td>\n",
       "      <td>-0.098698</td>\n",
       "      <td>-0.405518</td>\n",
       "      <td>0.629009</td>\n",
       "      <td>-1.205222</td>\n",
       "      <td>-0.989608</td>\n",
       "      <td>-1.052450</td>\n",
       "      <td>-1.082164</td>\n",
       "      <td>-0.551245</td>\n",
       "      <td>2.690250</td>\n",
       "      <td>-0.919279</td>\n",
       "      <td>-1.418809</td>\n",
       "      <td>-0.327146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58738</th>\n",
       "      <td>-1.123925</td>\n",
       "      <td>2.829232</td>\n",
       "      <td>0.281349</td>\n",
       "      <td>0.459110</td>\n",
       "      <td>-0.539199</td>\n",
       "      <td>0.814040</td>\n",
       "      <td>-0.162167</td>\n",
       "      <td>0.123279</td>\n",
       "      <td>-0.639389</td>\n",
       "      <td>1.866671</td>\n",
       "      <td>-1.661150</td>\n",
       "      <td>-1.217272</td>\n",
       "      <td>-0.752796</td>\n",
       "      <td>0.835984</td>\n",
       "      <td>-0.637278</td>\n",
       "      <td>-0.090449</td>\n",
       "      <td>2.143275</td>\n",
       "      <td>1.077346</td>\n",
       "      <td>-0.202596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58739</th>\n",
       "      <td>-0.419711</td>\n",
       "      <td>0.062863</td>\n",
       "      <td>1.067361</td>\n",
       "      <td>-0.669245</td>\n",
       "      <td>-0.445627</td>\n",
       "      <td>0.173409</td>\n",
       "      <td>1.245322</td>\n",
       "      <td>1.689096</td>\n",
       "      <td>-1.082880</td>\n",
       "      <td>1.649623</td>\n",
       "      <td>-0.570011</td>\n",
       "      <td>-0.560612</td>\n",
       "      <td>-1.209715</td>\n",
       "      <td>0.385461</td>\n",
       "      <td>-0.904895</td>\n",
       "      <td>0.965109</td>\n",
       "      <td>0.192485</td>\n",
       "      <td>-0.500860</td>\n",
       "      <td>0.058454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58740 rows × 19 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow import keras\n",
    "AoModel = keras.Sequential([\n",
    "     layers.InputLayer(19, name=\"digits\"),\n",
    "     layers.Dense(2000, activation=\"relu\", name = \"Layer1\"),\n",
    "     #keras.layers.Dropout(0.5),\n",
    "     #keras.layers.BatchNormalization(),\n",
    "     layers.Dense(1050, activation=\"relu\", name=\"Layer2\"),\n",
    "     #keras.layers.Dropout(0.5),\n",
    "     #keras.layers.BatchNormalization(),\n",
    "     layers.Dense(100, activation=\"relu\", name=\"Layer3\"),\n",
    "     #keras.layers.BatchNormalization(),\n",
    "     layers.Dense(9, activation=\"linear\", name=\"predictions\"),\n",
    "\n",
    "])\n",
    "keras.utils.plot_model(AoModel, \"AoModel.png\", show_shapes = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build batchnorm model "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow import keras\n",
    "AoBatchNormModel = keras.Sequential([\n",
    "     layers.BatchNormalization(19, name=\"digits\"),\n",
    "     layers.Dense(2000, activation=\"relu\", name = \"Layer1\"),\n",
    "     #keras.layers.Dropout(0.5),\n",
    "     #keras.layers.BatchNormalization(),\n",
    "     layers.Dense(1050, activation=\"relu\", name=\"Layer2\"),\n",
    "     #keras.layers.Dropout(0.5),\n",
    "     #keras.layers.BatchNormalization(),\n",
    "     layers.Dense(100, activation=\"relu\", name=\"Layer3\"),\n",
    "     #keras.layers.BatchNormalization(),\n",
    "     layers.Dense(9, activation=\"linear\", name=\"predictions\"),\n",
    "\n",
    "])\n",
    "#keras.utils.plot_model(AoAoBatchNormModelModel, \"AoModel.png\", show_shapes = True)\n",
    "#AoBatchNormModel.summary()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow_addons as tfa\n",
    "opt = tf.ad"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build model with funcitonal API"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NoLayers =4\n",
    "inputs = keras.Input(shape=(19,))\n",
    "Layer1 = layers.Dense(2000, activation=\"relu\", name = \"Layer1\")\n",
    "x = Layer1(inputs)\n",
    "x = layers.Dense(1050, activation=\"relu\", name = \"Layer2\")(x)\n",
    "x = layers.Dense(1000, activation=\"relu\", name = \"Layer3\")(x)\n",
    "if(NoLayers >3):\n",
    "    x = layers.Dense(100, activation=\"relu\", name = \"Layer4\")(x)\n",
    "outputs = layers.Dense(9, name = \"Output\")(x)\n",
    "AoFunctionalModel = keras.Model(inputs=inputs, outputs=outputs, name=\"AOModel\")\n",
    "\n",
    "AoFunctionalModel.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keras.utils.plot_model(AoFunctionalModel, \"AoModel.png\", show_shapes = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Ao funcitonal model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "#tf.random.set_seed(2021)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.000001, verbose=1, cooldown= 5)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "AoFunctionalModel.compile(loss= keras.losses.MeanSquaredError(),  optimizer=opt, metrics= [tf.keras.metrics.RootMeanSquaredError()])\n",
    "AoFunctionalModel.fit(x_train_norm, y_train, batch_size=batch_size, epochs=epochs,validation_split = 0.2, verbose = 2, callbacks=[reduce_lr])\n",
    "\n",
    "#AoModel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[WandbCallback()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9 different last hidden layers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "def custom_activation(x):\n",
    "    return (K.sigmoid(x) - 0.5) *2\n",
    "get_custom_objects().update({'linear': Activation(custom_activation)})\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(19,))\n",
    "Layer1 = layers.Dense(2000, activation=\"linear\", name = \"Layer1\")\n",
    "x = Layer1(inputs)\n",
    "x = layers.Dense(1050, activation=\"relu\", name = \"Layer2\")(x)\n",
    "\n",
    "# create 9 outputs\n",
    "x1 = layers.Dense(100, activation=\"relu\", name = \"Layer3_1\")(x)\n",
    "out1 = layers.Dense(1, activation=custom_activation, name = \"out1\")(x1)\n",
    "\n",
    "x2 = layers.Dense(100, activation=\"relu\", name = \"Layer3_2\")(x)\n",
    "out2 = layers.Dense(1, activation=custom_activation, name = \"out2\")(x2)\n",
    "\n",
    "x3 = layers.Dense(100, activation=\"relu\", name = \"Layer3_3\")(x)\n",
    "out3 = layers.Dense(1, activation=custom_activation, name = \"out3\")(x3)\n",
    "\n",
    "x4 = layers.Dense(100, activation=\"relu\", name = \"Layer3_4\")(x)\n",
    "out4 = layers.Dense(1, activation=custom_activation, name = \"out4\")(x4)\n",
    "\n",
    "x5 = layers.Dense(100, activation=\"relu\", name = \"Layer3_5\")(x)\n",
    "out5 = layers.Dense(1, activation=custom_activation, name = \"out5\")(x5)\n",
    "\n",
    "x6 = layers.Dense(100, activation=\"relu\", name = \"Layer3_6\")(x)\n",
    "out6 = layers.Dense(1, activation=custom_activation, name = \"out6\")(x6)\n",
    "\n",
    "x7 = layers.Dense(100, activation=\"relu\", name = \"Layer3_7\")(x)\n",
    "out7 = layers.Dense(1, activation=custom_activation, name = \"out7\")(x7)\n",
    "\n",
    "x8 = layers.Dense(100, activation=\"relu\", name = \"Layer3_8\")(x)\n",
    "out8 = layers.Dense(1, activation=custom_activation, name = \"out8\")(x8)\n",
    "\n",
    "x9 = layers.Dense(100, activation=\"relu\", name = \"Layer3_9\")(x)\n",
    "out9 = layers.Dense(1, activation=custom_activation, name = \"out9\")(x9)\n",
    "\n",
    "\n",
    "outputs = tf.keras.layers.Concatenate(axis=1)([out1,out2,out3,out4,out5,out6,out7,out8,out9])\n",
    "\n",
    "\n",
    "#outputs = layers.Dense(9, name = \"Output\")(x)\n",
    "Ao9Exits = keras.Model(inputs=inputs, outputs=outputs, name=\"AOModel\")\n",
    "\n",
    "Ao9Exits.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "keras.utils.plot_model(AoFunctionalModel, \"AoModel.png\", show_shapes = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "tf.random.set_seed(2021)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.000001, verbose=1, cooldown= 5)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "Ao9Exits.compile(loss= keras.losses.MeanSquaredError(),  optimizer=opt, metrics= [tf.keras.metrics.RootMeanSquaredError()])\n",
    "Ao9Exits.fit(x_train_norm, y_train, batch_size=batch_size, epochs=epochs,validation_split = 0.2, verbose = 2, callbacks=[reduce_lr])\n",
    "\n",
    "#AoModel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[WandbCallback()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Batchnorm model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NoLayers =4\n",
    "inputs = keras.Input(shape=(19,))\n",
    "BatchNorm1 = layers.BatchNormalization()\n",
    "Layer1 = layers.Dense(2000, activation=\"relu\", name = \"Layer1\")\n",
    "x = BatchNorm1(inputs)\n",
    "x = Layer1(x)\n",
    "x = layers.Dense(1050, activation=\"relu\", name = \"Layer2\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(1000, activation=\"relu\", name = \"Layer3\")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "\n",
    "if(NoLayers >3):\n",
    "    x = layers.Dense(100, activation=\"relu\", name = \"Layer4\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "outputs = layers.Dense(9, name = \"Output\")(x)\n",
    "AoBatchNormModel = keras.Model(inputs=inputs, outputs=outputs, name=\"AOModel\")\n",
    "\n",
    "AoBatchNormModel.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train batchNorm model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "#tf.random.set_seed(2021)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.000001, verbose=1, cooldown= 5)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "AoBatchNormModel.compile(loss= keras.losses.MeanSquaredError(),  optimizer=opt, metrics= [tf.keras.metrics.RootMeanSquaredError()])\n",
    "AoBatchNormModel.fit(x_train_norm, y_train, batch_size=batch_size, epochs=epochs,validation_split = 0.2, verbose = 2, callbacks=[reduce_lr])\n",
    "\n",
    "#AoModel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[WandbCallback()])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build COnv1D model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "conv1D_1 =64\n",
    "conv1D_1_width = 4\n",
    "conv1D_2 = 32\n",
    "conv1D_2_width = 2\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (4, 19, 1)\n",
    "\n",
    "AoConv1DModel = keras.Sequential([\n",
    "     \n",
    "     \n",
    "     #keras.layers.Conv1D(32, 3, activation='relu',input_shape=(None,19), name=\"conv1d\"),\n",
    "     keras.layers.Conv1D( conv1D_1, conv1D_1_width, activation='relu',input_shape=input_shape[1:]),\n",
    "     keras.layers.MaxPool1D(pool_size=2),\n",
    "     #keras.layers.Dropout(rate = 0.2),\n",
    "     keras.layers.Conv1D( conv1D_2, conv1D_2_width, activation='relu',input_shape=input_shape[1:]),\n",
    "     keras.layers.MaxPool1D(pool_size=2),\n",
    "\n",
    "\n",
    "     #keras.layers.Dropout(rate = 0.2),\n",
    "     keras.layers.Flatten(),\n",
    "     keras.layers.Dense(2000, activation=\"relu\"),\n",
    "     #keras.layers.Dropout(rate = 0.2),\n",
    "     keras.layers.Dense(1050, activation=\"relu\"),\n",
    "     #keras.layers.Dropout(rate = 0.1),\n",
    "\n",
    "     keras.layers.Dense(100, activation=\"relu\"),\n",
    "     #keras.layers.Dropout(rate = 0.2),\n",
    "\n",
    "     keras.layers.Dense(9,  name=\"predictions\"),\n",
    "\n",
    "])\n",
    "AoConv1DModel.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/uwe/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 16, 64)            320       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 7, 32)             4128      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 3, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2000)              194000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1050)              2101050   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               105100    \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 9)                 909       \n",
      "=================================================================\n",
      "Total params: 2,405,507\n",
      "Trainable params: 2,405,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reshape training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "input_train = x_train_norm.to_numpy()[:,:,np.newaxis]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train simple model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Conv1D model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "#tf.random.set_seed(2021)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.000001, verbose=1, cooldown= 5)\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "AoConv1DModel.compile(loss= keras.losses.MeanSquaredError(),  optimizer=opt, metrics= [tf.keras.metrics.RootMeanSquaredError()])\n",
    "AoConv1DModel.fit(input_train, y_train, batch_size=batch_size, epochs=epochs,validation_split = 0.2, verbose = 2, callbacks=[reduce_lr])\n",
    "\n",
    "#AoModel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[WandbCallback()])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 46992 samples, validate on 11748 samples\n",
      "Epoch 1/200\n",
      "46992/46992 - 3s - loss: 0.0068 - root_mean_squared_error: 0.0822 - val_loss: 0.0039 - val_root_mean_squared_error: 0.0621\n",
      "Epoch 2/200\n",
      "46992/46992 - 3s - loss: 0.0026 - root_mean_squared_error: 0.0509 - val_loss: 0.0023 - val_root_mean_squared_error: 0.0482\n",
      "Epoch 3/200\n",
      "46992/46992 - 3s - loss: 0.0019 - root_mean_squared_error: 0.0436 - val_loss: 0.0019 - val_root_mean_squared_error: 0.0432\n",
      "Epoch 4/200\n",
      "46992/46992 - 3s - loss: 0.0015 - root_mean_squared_error: 0.0390 - val_loss: 0.0019 - val_root_mean_squared_error: 0.0441\n",
      "Epoch 5/200\n",
      "46992/46992 - 3s - loss: 0.0014 - root_mean_squared_error: 0.0368 - val_loss: 0.0016 - val_root_mean_squared_error: 0.0400\n",
      "Epoch 6/200\n",
      "46992/46992 - 3s - loss: 0.0012 - root_mean_squared_error: 0.0344 - val_loss: 0.0017 - val_root_mean_squared_error: 0.0417\n",
      "Epoch 7/200\n",
      "46992/46992 - 3s - loss: 0.0011 - root_mean_squared_error: 0.0330 - val_loss: 0.0016 - val_root_mean_squared_error: 0.0398\n",
      "Epoch 8/200\n",
      "46992/46992 - 3s - loss: 0.0010 - root_mean_squared_error: 0.0319 - val_loss: 0.0014 - val_root_mean_squared_error: 0.0374\n",
      "Epoch 9/200\n",
      "46992/46992 - 3s - loss: 9.4548e-04 - root_mean_squared_error: 0.0307 - val_loss: 0.0015 - val_root_mean_squared_error: 0.0390\n",
      "Epoch 10/200\n",
      "46992/46992 - 3s - loss: 8.8817e-04 - root_mean_squared_error: 0.0298 - val_loss: 0.0015 - val_root_mean_squared_error: 0.0383\n",
      "Epoch 11/200\n",
      "46992/46992 - 3s - loss: 8.3892e-04 - root_mean_squared_error: 0.0290 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0344\n",
      "Epoch 12/200\n",
      "46992/46992 - 3s - loss: 7.9927e-04 - root_mean_squared_error: 0.0283 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0340\n",
      "Epoch 13/200\n",
      "46992/46992 - 3s - loss: 7.4970e-04 - root_mean_squared_error: 0.0274 - val_loss: 0.0012 - val_root_mean_squared_error: 0.0346\n",
      "Epoch 14/200\n",
      "46992/46992 - 3s - loss: 7.3629e-04 - root_mean_squared_error: 0.0271 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0319\n",
      "Epoch 15/200\n",
      "46992/46992 - 3s - loss: 6.9037e-04 - root_mean_squared_error: 0.0263 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0317\n",
      "Epoch 16/200\n",
      "46992/46992 - 3s - loss: 6.6897e-04 - root_mean_squared_error: 0.0259 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0326\n",
      "Epoch 17/200\n",
      "46992/46992 - 3s - loss: 6.2797e-04 - root_mean_squared_error: 0.0251 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0330\n",
      "Epoch 18/200\n",
      "46992/46992 - 3s - loss: 6.1907e-04 - root_mean_squared_error: 0.0249 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0319\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "46992/46992 - 3s - loss: 6.2314e-04 - root_mean_squared_error: 0.0250 - val_loss: 0.0010 - val_root_mean_squared_error: 0.0323\n",
      "Epoch 20/200\n",
      "46992/46992 - 3s - loss: 3.0877e-04 - root_mean_squared_error: 0.0176 - val_loss: 7.0501e-04 - val_root_mean_squared_error: 0.0266\n",
      "Epoch 21/200\n",
      "46992/46992 - 3s - loss: 2.5113e-04 - root_mean_squared_error: 0.0158 - val_loss: 6.7707e-04 - val_root_mean_squared_error: 0.0260\n",
      "Epoch 22/200\n",
      "46992/46992 - 3s - loss: 2.3433e-04 - root_mean_squared_error: 0.0153 - val_loss: 6.9097e-04 - val_root_mean_squared_error: 0.0263\n",
      "Epoch 23/200\n",
      "46992/46992 - 3s - loss: 2.2351e-04 - root_mean_squared_error: 0.0150 - val_loss: 7.0398e-04 - val_root_mean_squared_error: 0.0265\n",
      "Epoch 24/200\n",
      "46992/46992 - 3s - loss: 2.1454e-04 - root_mean_squared_error: 0.0146 - val_loss: 7.2453e-04 - val_root_mean_squared_error: 0.0269\n",
      "Epoch 25/200\n",
      "46992/46992 - 3s - loss: 2.0768e-04 - root_mean_squared_error: 0.0144 - val_loss: 7.2826e-04 - val_root_mean_squared_error: 0.0270\n",
      "Epoch 26/200\n",
      "46992/46992 - 3s - loss: 1.9924e-04 - root_mean_squared_error: 0.0141 - val_loss: 6.9607e-04 - val_root_mean_squared_error: 0.0264\n",
      "Epoch 27/200\n",
      "46992/46992 - 3s - loss: 1.9415e-04 - root_mean_squared_error: 0.0139 - val_loss: 6.7845e-04 - val_root_mean_squared_error: 0.0260\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "46992/46992 - 3s - loss: 1.8772e-04 - root_mean_squared_error: 0.0137 - val_loss: 7.2165e-04 - val_root_mean_squared_error: 0.0269\n",
      "Epoch 29/200\n",
      "46992/46992 - 3s - loss: 1.4518e-04 - root_mean_squared_error: 0.0120 - val_loss: 6.4610e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 30/200\n",
      "46992/46992 - 3s - loss: 1.3856e-04 - root_mean_squared_error: 0.0118 - val_loss: 6.4710e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 31/200\n",
      "46992/46992 - 3s - loss: 1.3676e-04 - root_mean_squared_error: 0.0117 - val_loss: 6.4543e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 32/200\n",
      "46992/46992 - 3s - loss: 1.3498e-04 - root_mean_squared_error: 0.0116 - val_loss: 6.4432e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 33/200\n",
      "46992/46992 - 3s - loss: 1.3340e-04 - root_mean_squared_error: 0.0115 - val_loss: 6.4559e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 34/200\n",
      "46992/46992 - 3s - loss: 1.3152e-04 - root_mean_squared_error: 0.0115 - val_loss: 6.4505e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 35/200\n",
      "46992/46992 - 3s - loss: 1.3016e-04 - root_mean_squared_error: 0.0114 - val_loss: 6.3920e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 36/200\n",
      "46992/46992 - 3s - loss: 1.2842e-04 - root_mean_squared_error: 0.0113 - val_loss: 6.4337e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "46992/46992 - 3s - loss: 1.2701e-04 - root_mean_squared_error: 0.0113 - val_loss: 6.4211e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 38/200\n",
      "46992/46992 - 3s - loss: 1.1838e-04 - root_mean_squared_error: 0.0109 - val_loss: 6.4160e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 39/200\n",
      "46992/46992 - 3s - loss: 1.1737e-04 - root_mean_squared_error: 0.0108 - val_loss: 6.4350e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 40/200\n",
      "46992/46992 - 3s - loss: 1.1694e-04 - root_mean_squared_error: 0.0108 - val_loss: 6.4207e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 41/200\n",
      "46992/46992 - 3s - loss: 1.1655e-04 - root_mean_squared_error: 0.0108 - val_loss: 6.3770e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 42/200\n",
      "46992/46992 - 3s - loss: 1.1628e-04 - root_mean_squared_error: 0.0108 - val_loss: 6.5121e-04 - val_root_mean_squared_error: 0.0255\n",
      "Epoch 43/200\n",
      "46992/46992 - 3s - loss: 1.1588e-04 - root_mean_squared_error: 0.0108 - val_loss: 6.4180e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 44/200\n",
      "46992/46992 - 3s - loss: 1.1554e-04 - root_mean_squared_error: 0.0107 - val_loss: 6.4128e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 45/200\n",
      "46992/46992 - 3s - loss: 1.1519e-04 - root_mean_squared_error: 0.0107 - val_loss: 6.4831e-04 - val_root_mean_squared_error: 0.0255\n",
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "46992/46992 - 3s - loss: 1.1481e-04 - root_mean_squared_error: 0.0107 - val_loss: 6.4346e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 47/200\n",
      "46992/46992 - 3s - loss: 1.1298e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4166e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 48/200\n",
      "46992/46992 - 3s - loss: 1.1277e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4015e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 49/200\n",
      "46992/46992 - 3s - loss: 1.1268e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4258e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 50/200\n",
      "46992/46992 - 3s - loss: 1.1261e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4096e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 51/200\n",
      "46992/46992 - 3s - loss: 1.1253e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.3923e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 52/200\n",
      "46992/46992 - 3s - loss: 1.1246e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4048e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 53/200\n",
      "46992/46992 - 3s - loss: 1.1240e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.3871e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 54/200\n",
      "46992/46992 - 3s - loss: 1.1233e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4173e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "46992/46992 - 3s - loss: 1.1226e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4099e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 56/200\n",
      "46992/46992 - 3s - loss: 1.1201e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4190e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 57/200\n",
      "46992/46992 - 3s - loss: 1.1197e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.3960e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 58/200\n",
      "46992/46992 - 3s - loss: 1.1193e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.3989e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 59/200\n",
      "46992/46992 - 3s - loss: 1.1188e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.3930e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 60/200\n",
      "46992/46992 - 3s - loss: 1.1183e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4320e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 61/200\n",
      "46992/46992 - 3s - loss: 1.1180e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4379e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 62/200\n",
      "46992/46992 - 3s - loss: 1.1175e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4104e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 63/200\n",
      "46992/46992 - 3s - loss: 1.1170e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.3956e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 64/200\n",
      "46992/46992 - 3s - loss: 1.1166e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4415e-04 - val_root_mean_squared_error: 0.0254\n",
      "Epoch 65/200\n",
      "46992/46992 - 3s - loss: 1.1162e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4072e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 66/200\n",
      "46992/46992 - 3s - loss: 1.1157e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4185e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 67/200\n",
      "46992/46992 - 3s - loss: 1.1153e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4174e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 68/200\n",
      "46992/46992 - 3s - loss: 1.1149e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4077e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 69/200\n",
      "46992/46992 - 3s - loss: 1.1145e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.3932e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 70/200\n",
      "46992/46992 - 3s - loss: 1.1140e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4190e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 71/200\n",
      "46992/46992 - 3s - loss: 1.1137e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.4133e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 72/200\n",
      "46992/46992 - 3s - loss: 1.1132e-04 - root_mean_squared_error: 0.0106 - val_loss: 6.3953e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 73/200\n",
      "46992/46992 - 3s - loss: 1.1129e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4161e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 74/200\n",
      "46992/46992 - 3s - loss: 1.1124e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3887e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 75/200\n",
      "46992/46992 - 3s - loss: 1.1120e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4007e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 76/200\n",
      "46992/46992 - 3s - loss: 1.1115e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4065e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 77/200\n",
      "46992/46992 - 3s - loss: 1.1111e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4172e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 78/200\n",
      "46992/46992 - 3s - loss: 1.1107e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4007e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 79/200\n",
      "46992/46992 - 3s - loss: 1.1103e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4091e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 80/200\n",
      "46992/46992 - 3s - loss: 1.1098e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3957e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 81/200\n",
      "46992/46992 - 3s - loss: 1.1094e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3877e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 82/200\n",
      "46992/46992 - 3s - loss: 1.1089e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4139e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 83/200\n",
      "46992/46992 - 3s - loss: 1.1086e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3920e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 84/200\n",
      "46992/46992 - 3s - loss: 1.1082e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4089e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 85/200\n",
      "46992/46992 - 3s - loss: 1.1078e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4130e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 86/200\n",
      "46992/46992 - 3s - loss: 1.1074e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4141e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 87/200\n",
      "46992/46992 - 3s - loss: 1.1069e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4151e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 88/200\n",
      "46992/46992 - 3s - loss: 1.1065e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4033e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 89/200\n",
      "46992/46992 - 3s - loss: 1.1061e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4173e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 90/200\n",
      "46992/46992 - 3s - loss: 1.1057e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3847e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 91/200\n",
      "46992/46992 - 3s - loss: 1.1053e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3982e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 92/200\n",
      "46992/46992 - 3s - loss: 1.1048e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3999e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 93/200\n",
      "46992/46992 - 3s - loss: 1.1044e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3995e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 94/200\n",
      "46992/46992 - 3s - loss: 1.1039e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4146e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 95/200\n",
      "46992/46992 - 3s - loss: 1.1037e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4038e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 96/200\n",
      "46992/46992 - 3s - loss: 1.1032e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4156e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 97/200\n",
      "46992/46992 - 3s - loss: 1.1029e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3821e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 98/200\n",
      "46992/46992 - 3s - loss: 1.1024e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3979e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 99/200\n",
      "46992/46992 - 3s - loss: 1.1020e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4091e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 100/200\n",
      "46992/46992 - 3s - loss: 1.1015e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4124e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 101/200\n",
      "46992/46992 - 3s - loss: 1.1011e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4077e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 102/200\n",
      "46992/46992 - 3s - loss: 1.1008e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4075e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 103/200\n",
      "46992/46992 - 3s - loss: 1.1003e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4132e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 104/200\n",
      "46992/46992 - 3s - loss: 1.0999e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4002e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 105/200\n",
      "46992/46992 - 3s - loss: 1.0995e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3863e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 106/200\n",
      "46992/46992 - 3s - loss: 1.0991e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4131e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 107/200\n",
      "46992/46992 - 3s - loss: 1.0987e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4174e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 108/200\n",
      "46992/46992 - 3s - loss: 1.0983e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4045e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 109/200\n",
      "46992/46992 - 3s - loss: 1.0978e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4017e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 110/200\n",
      "46992/46992 - 3s - loss: 1.0975e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4067e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 111/200\n",
      "46992/46992 - 3s - loss: 1.0971e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4060e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 112/200\n",
      "46992/46992 - 3s - loss: 1.0967e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3960e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 113/200\n",
      "46992/46992 - 3s - loss: 1.0962e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3945e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 114/200\n",
      "46992/46992 - 3s - loss: 1.0959e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4067e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 115/200\n",
      "46992/46992 - 3s - loss: 1.0955e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4068e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 116/200\n",
      "46992/46992 - 3s - loss: 1.0950e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3730e-04 - val_root_mean_squared_error: 0.0252\n",
      "Epoch 117/200\n",
      "46992/46992 - 3s - loss: 1.0947e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3986e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 118/200\n",
      "46992/46992 - 3s - loss: 1.0943e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3874e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 119/200\n",
      "46992/46992 - 3s - loss: 1.0939e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4002e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 120/200\n",
      "46992/46992 - 3s - loss: 1.0934e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.3909e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 121/200\n",
      "46992/46992 - 3s - loss: 1.0931e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4229e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 122/200\n",
      "46992/46992 - 3s - loss: 1.0927e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4029e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 123/200\n",
      "46992/46992 - 3s - loss: 1.0922e-04 - root_mean_squared_error: 0.0105 - val_loss: 6.4098e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 124/200\n",
      "46992/46992 - 3s - loss: 1.0919e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.3938e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 125/200\n",
      "46992/46992 - 3s - loss: 1.0914e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.4057e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 126/200\n",
      "46992/46992 - 3s - loss: 1.0912e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.3977e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 127/200\n",
      "46992/46992 - 3s - loss: 1.0907e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.4175e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 128/200\n",
      "46992/46992 - 3s - loss: 1.0902e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.3930e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 129/200\n",
      "46992/46992 - 3s - loss: 1.0899e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.4131e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 130/200\n",
      "46992/46992 - 3s - loss: 1.0895e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.4150e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 131/200\n",
      "46992/46992 - 3s - loss: 1.0891e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.4037e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 132/200\n",
      "46992/46992 - 3s - loss: 1.0886e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.3692e-04 - val_root_mean_squared_error: 0.0252\n",
      "Epoch 133/200\n",
      "46992/46992 - 3s - loss: 1.0883e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.4066e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 134/200\n",
      "46992/46992 - 3s - loss: 1.0878e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.3962e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 135/200\n",
      "46992/46992 - 3s - loss: 1.0875e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.4102e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 136/200\n",
      "46992/46992 - 3s - loss: 1.0872e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.4050e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 137/200\n",
      "46992/46992 - 3s - loss: 1.0868e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.4013e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 138/200\n",
      "46992/46992 - 3s - loss: 1.0864e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.3892e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 139/200\n",
      "46992/46992 - 3s - loss: 1.0858e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.3941e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 140/200\n",
      "46992/46992 - 3s - loss: 1.0856e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.3777e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 141/200\n",
      "46992/46992 - 3s - loss: 1.0852e-04 - root_mean_squared_error: 0.0104 - val_loss: 6.3970e-04 - val_root_mean_squared_error: 0.0253\n",
      "Epoch 142/200\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-42a10f358974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mAoConv1DModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRootMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mAoConv1DModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#AoModel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[WandbCallback()])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu-cuda8/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7240fa07bd5f0492db6e5998fcde1467c49f289639e3a06c91ae7c487c9ff707"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}