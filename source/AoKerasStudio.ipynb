{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIQxWeVW4Pj"
      },
      "source": [
        "# üöÄ Install, Import, and Log in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmlhYoBlOxYd"
      },
      "source": [
        "### Step 0Ô∏è‚É£: Install W&B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAZBTFNQq0ys"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "#!pip install wandb\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu-KHLAeO3wi"
      },
      "source": [
        "### Step 1Ô∏è‚É£: Import W&B and Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkWTZxIyysDm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "import pickle5 as pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "#%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmVkQbw_q_07"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.init(config={\"hyper\": \"parameter\"})\n",
        "\n",
        "\n",
        "!wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "fluxData_df =pd.read_hdf(\"data/fluxData_df.h5\", key=\"fluxData_df\")\n",
        "zernikeData_df =pd.read_hdf(\"data/zernikeData_df.h5\", key=\"zernikeData_df\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNw4gP_Oqx7c"
      },
      "source": [
        "Read data in"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nhe7pVMlwv4H"
      },
      "source": [
        "# üë©‚Äçüç≥ Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnxsNbPmMvqr"
      },
      "outputs": [],
      "source": [
        "# Prepare the training dataset\n",
        "X = fluxData_df\n",
        "y = zernikeData_df\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build simple Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AoModel = keras.Sequential([\n",
        "     keras.layers.InputLayer(19, name=\"digits\"),\n",
        "     keras.layers.Dense(2000, activation=\"relu\"),\n",
        "     keras.layers.Dense(1050, activation=\"relu\"),\n",
        "     keras.layers.Dense(100, activation=\"relu\"),\n",
        "     keras.layers.Dense(9, activation=\"linear\", name=\"predictions\"),\n",
        "\n",
        "])\n",
        "AoModel.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "\n",
        "    # optimizer='sgd',\n",
        "    # loss='mse',\n",
        "    # metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "\n",
        "\n",
        "AoModel.compile(loss= keras.losses.MeanSquaredError(), optimizer=\"adam\", metrics= [tf.keras.metrics.RootMeanSquaredError()])\n",
        "AoModel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose = 2,  callbacks=[WandbCallback()])\n",
        "\n",
        "#AoModel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[WandbCallback()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftU5d82stSdV"
      },
      "source": [
        "# üß† Define the Model and Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBQZ87FiXkFl"
      },
      "source": [
        "## üèóÔ∏è Build a NN according to paper \"An all-photonic focal-plane wavefront sensor\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd8GpFudzEiU"
      },
      "outputs": [],
      "source": [
        "def Model():\n",
        "    inputs = keras.Input(shape=(19,), name=\"digits\")\n",
        "    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n",
        "    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n",
        "    outputs = keras.layers.Dense(9, name=\"predictions\")(x2)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    \n",
        "def train_step(x, y, model, optimizer, loss_fn, train_acc_metric):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(x, training=True)\n",
        "        \n",
        "        m = tf.keras.metrics.RootMeanSquaredError()\n",
        "        #m.reset_state()\n",
        "        m.update_state(y, y_pred)\n",
        "        #m.result().numpy()\n",
        "        loss_value = keras.metrics.RootMeanSquaredError(y, y_pred)\n",
        "        #loss_value = m.result().numpy()\n",
        "        #loss_value = keras.losses.MeanSquaredError(y, y_pred)\n",
        "\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "   # train_acc_metric.update_state(y, logits)\n",
        "\n",
        "    return loss_value\n",
        "\n",
        "    \n",
        "def test_step(x, y, model, loss_fn, val_acc_metric):\n",
        "    y_pred = model(x, training=True)\n",
        "    m = tf.keras.metrics.RootMeanSquaredError()\n",
        "    #m.reset_state()\n",
        "    m.update_state(y, y_pred)\n",
        "    #m.result().numpy()\n",
        "       # loss_value = keras.metrics.RootMeanSquaredError(y, y_pred)\n",
        "    # loss_value = m.result().numpy()\n",
        "    #loss_value = keras.losses.MeanSquaredError(y, y_pred)\n",
        "\n",
        "    return loss_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDzAX1Uc82Ll"
      },
      "source": [
        "## üîÅ Write a Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Qe-ZTcYiAn"
      },
      "source": [
        "### Step 3Ô∏è‚É£: Log metrics with `wandb.log`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb12E4zoBDOy"
      },
      "outputs": [],
      "source": [
        "def train(train_dataset,\n",
        "          val_dataset, \n",
        "          model,\n",
        "          optimizer,\n",
        "          loss_fn,\n",
        "          train_acc_metric,\n",
        "          val_acc_metric,\n",
        "          epochs=2, \n",
        "          log_step=200, \n",
        "          val_log_step=50):\n",
        "  \n",
        "    for epoch in range(epochs):\n",
        "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "        train_loss = []   \n",
        "        val_loss = []\n",
        "\n",
        "        # Iterate over the batches of the dataset\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "            loss_value = train_step(x_batch_train, y_batch_train, \n",
        "                                    model, optimizer, \n",
        "                                    loss_fn, train_acc_metric)\n",
        "            train_loss.append(float(loss_value))\n",
        "\n",
        "        # Run a validation loop at the end of each epoch\n",
        "        for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
        "            val_loss_value = test_step(x_batch_val, y_batch_val, \n",
        "                                       model, loss_fn, \n",
        "                                       val_acc_metric)\n",
        "            val_loss.append(float(val_loss_value))\n",
        "            \n",
        "        # Display metrics at the end of each epoch\n",
        "        train_acc = train_acc_metric.result()\n",
        "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "        val_acc = val_acc_metric.result()\n",
        "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "\n",
        "        # Reset metrics at the end of each epoch\n",
        "        train_acc_metric.reset_states()\n",
        "        val_acc_metric.reset_states()\n",
        "\n",
        "        # 3Ô∏è‚É£ log metrics using wandb.log\n",
        "        wandb.log({'epochs': epoch,\n",
        "                   'loss': np.mean(train_loss),\n",
        "                   'acc': float(train_acc), \n",
        "                   'val_loss': np.mean(val_loss),\n",
        "                   'val_acc':float(val_acc)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIkUJh4XB-10"
      },
      "source": [
        "# Step 4Ô∏è‚É£: Configure the Sweep\n",
        "\n",
        "This is where you will:\n",
        "* Define the hyperparameters you're sweeping over\n",
        "* Provide your hyperparameter optimization method. We have `random`, `grid` and `bayes` methods.\n",
        "* Provide an objective and a `metric` if using `bayes`, for example to `minimize` the `val_loss`.\n",
        "* Use `hyperband` for early termination of poorly-performing runs\n",
        "\n",
        "#### [Check out more on Sweep Configs $\\rightarrow$](https://docs.wandb.com/sweeps/configuration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI_JI2HBZs10"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "  'method': 'bayes', \n",
        "  'metric': {\n",
        "      'name': 'val_loss',\n",
        "      'goal': 'minimize'\n",
        "  },\n",
        "  'early_terminate':{\n",
        "      'type': 'hyperband',\n",
        "      'min_iter': 5\n",
        "  },\n",
        "  'parameters': {\n",
        "      'batch_size': {\n",
        "          'values': [8, ]\n",
        "      },\n",
        "      'learning_rate':{\n",
        "          'values': [0.01]\n",
        "      }\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXRNVUqia07w"
      },
      "source": [
        "# Step 5Ô∏è‚É£: Wrap the Training Loop\n",
        "\n",
        "You'll need a function, like `sweep_train` below,\n",
        "that uses `wandb.config` to set the hyperparameters\n",
        "before `train` gets called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98pkZByxC4Pl"
      },
      "outputs": [],
      "source": [
        "def sweep_train():\n",
        "    # Specify the hyperparameter to be tuned along with\n",
        "    # an initial value\n",
        "    config_defaults = {\n",
        "        'batch_size': 8,\n",
        "        'learning_rate': 0.01\n",
        "    }\n",
        "\n",
        "    # Initialize wandb with a sample project name\n",
        "    wandb.init(config=config_defaults)  # this gets over-written in the Sweep\n",
        "\n",
        "    # Specify the other hyperparameters to the configuration, if any\n",
        "    wandb.config.epochs = 7\n",
        "    wandb.config.log_step = 200\n",
        "    wandb.config.val_log_step = 50\n",
        "    wandb.config.architecture_name = \"CNN\"\n",
        "    wandb.config.dataset_name = \"CIFAR-10\"\n",
        "\n",
        "    # build input pipeline using tf.data\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(wandb.config.batch_size)\n",
        "\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    val_dataset = val_dataset.batch(wandb.config.batch_size)\n",
        "\n",
        "    # initialize model\n",
        "    model = Model()\n",
        "\n",
        "    # Instantiate an optimizer to train the model.\n",
        "    optimizer = keras.optimizers.Adam(learning_rate= 0.01)\n",
        "    # Instantiate a loss function.\n",
        "    #loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    loss_fn = keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
        "\n",
        "    # Prepare the metrics.\n",
        "    train_acc_metric = keras.metrics.MeanSquaredError(name=\"mean_squared_error\", dtype=None)\n",
        "    val_acc_metric = keras.metrics.MeanSquaredError(name=\"mean_squared_error\", dtype=None)\n",
        "\n",
        "    train(train_dataset,\n",
        "          val_dataset, \n",
        "          model,\n",
        "          optimizer,\n",
        "          loss_fn,\n",
        "          train_acc_metric,\n",
        "          val_acc_metric,\n",
        "          epochs=wandb.config.epochs, \n",
        "          log_step=wandb.config.log_step, \n",
        "          val_log_step=wandb.config.val_log_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_defaults = {\n",
        "        'batch_size': 8,\n",
        "        'learning_rate': 0.01\n",
        "    }\n",
        "\n",
        "# Initialize wandb with a sample project name\n",
        "#wandb.init(config=config_defaults)  # this gets over-written in the Sweep\n",
        "\n",
        "# Specify the other hyperparameters to the configuration, if any\n",
        "wandb.config.epochs = 7\n",
        "wandb.config.log_step = 200\n",
        "wandb.config.val_log_step = 50\n",
        "wandb.config.architecture_name = \"CNN\"\n",
        "wandb.config.dataset_name = \"CIFAR-10\"\n",
        "\n",
        "# build input pipeline using tf.data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(wandb.config.batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "val_dataset = val_dataset.batch(wandb.config.batch_size)\n",
        "\n",
        "# initialize model\n",
        "model = Model()\n",
        "\n",
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = keras.optimizers.Adam(learning_rate= 0.01)\n",
        "# Instantiate a loss function.\n",
        "#loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "loss_fn = keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.MeanSquaredError(name=\"mean_squared_error\", dtype=None)\n",
        "val_acc_metric = keras.metrics.MeanSquaredError(name=\"mean_squared_error\", dtype=None)\n",
        "\n",
        "train(train_dataset,\n",
        "        val_dataset, \n",
        "        model,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        train_acc_metric,\n",
        "        val_acc_metric,\n",
        "        epochs=wandb.config.epochs, \n",
        "        log_step=wandb.config.log_step, \n",
        "        val_log_step=wandb.config.val_log_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOReo0keGlbf"
      },
      "source": [
        "# Step 6Ô∏è‚É£: Initialize Sweep and Run Agent "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWpCLwGqncdF"
      },
      "outputs": [],
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"sweeps-tensorflow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goZ3Z7A8nbpn"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, function=sweep_train, count=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALGl_CS0GCLJ"
      },
      "source": [
        "# üëÄ Visualize Results\n",
        "\n",
        "Click on the **Sweep URL** link above to see your live results.\n",
        "\n",
        "<img src=\"https://i.imgur.com/6eWHZhg.png\" alt=\"Sweep Result\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiunZZ3AGUY3"
      },
      "source": [
        "# üé® Example Gallery\n",
        "\n",
        "See examples of projects tracked and visualized with W&B in our [Gallery ‚Üí](https://app.wandb.ai/gallery)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWERarlyGL1V"
      },
      "source": [
        "# üìè Best Practices\n",
        "1. **Projects**: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\n",
        "2. **Groups**: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group='experiment-1')`\n",
        "3. **Tags**: Add tags to track your current baseline or production model.\n",
        "4. **Notes**: Type notes in the table to track the changes between runs.\n",
        "5. **Reports**: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY39JyftGYqA"
      },
      "source": [
        "# ü§ì Advanced Setup\n",
        "1. [Environment variables](https://docs.wandb.com/library/environment-variables): Set API keys in environment variables so you can run training on a managed cluster.\n",
        "2. [Offline mode](https://docs.wandb.com/library/technical-faq#can-i-run-wandb-offline): Use `dryrun` mode to train offline and sync results later.\n",
        "3. [On-prem](https://docs.wandb.com/self-hosted): Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Hyperparameter Optimization in TensorFlow using W&B Sweeps",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "480a8e7f8b1be068d409ea33b890b25d8a792f4fc9f032a903f32caf5295501b"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('tf-gpu-cuda8': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}