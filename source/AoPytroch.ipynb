{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš€ Setup\n",
        "\n"
      ],
      "metadata": {
        "id": "4sP5RVh2b7Tb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you've picked a `method` to try out new values of the hyperparameters,\n",
        "you need to define what those `parameters` are.\n",
        "\n",
        "Most of the time, this step is straightforward:\n",
        "you just give the `parameter` a name\n",
        "and specify a list of legal `values`\n",
        "of the parameter.\n",
        "\n",
        "For example, when we choose the `optimizer` for our network,\n",
        "there's only a finite number of options.\n",
        "Here we stick with the two most popular choices, `adam` and `sgd`.\n",
        "Even for hyperparameters that have potentially infinite options,\n",
        "it usually only makes sense to try out\n",
        "a few select `values`,\n",
        "as we do here with the hidden `layer_size` and `dropout`."
      ],
      "metadata": {
        "id": "_KqljukQiBty"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "\n",
        "\n",
        "    loader = build_dataset(64)\n",
        "    network = build_network()\n",
        "    optimizer = build_optimizer(network, \"adam\", 1e-3)\n",
        "\n",
        "    for epoch in range(100):\n",
        "        print(\" hello train\")\n",
        "        print(epoch)\n",
        "        error = train_epoch(network, loader, optimizer)\n",
        "        print(\" hello poch ende\")\n",
        "\n",
        "        print(error)\n",
        "    print(\"end of loop\")        "
      ],
      "outputs": [],
      "metadata": {
        "id": "r4VjKui20N3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build data loader"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import pyarrow.feather as feather\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the four pieces of our training procedure:\n",
        "`build_dataset`, `build_network`, `build_optimizer`, and `train_epoch`.\n",
        "\n",
        "All of these are a standard part of a basic PyTorch pipeline,\n",
        "and their implementation is unaffected by the use of W&B,\n",
        "so we won't comment on them."
      ],
      "metadata": {
        "id": "MB2x_e11Wzkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def build_dataset(batch_size):\n",
        "    fluxData_df = feather.read_feather('data/fluxData.feather')\n",
        "    zernikeData_df = feather.read_feather('data/zernikeData.feather')\n",
        "\n",
        "\n",
        "    train_target = torch.tensor(zernikeData_df.values.astype(np.float32))\n",
        "    train = torch.tensor(fluxData_df.values.astype(np.float32))\n",
        "\n",
        "    train_tensor = torch.utils.data.TensorDataset(train, train_target) \n",
        "    loader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
        "   \n",
        "\n",
        "\n",
        "    return loader\n",
        "\n",
        "def build_network():\n",
        "    network = nn.Sequential( \n",
        "    nn.Linear(19,2000), nn.ReLU(),\n",
        "    nn.Linear(2000,1050), nn.ReLU(),\n",
        "    nn.Linear(1050,100), nn.ReLU(),\n",
        "    nn.Linear(100, 9))\n",
        "\n",
        "    return network.to(device)\n",
        "        \n",
        "\n",
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(network.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9)\n",
        "    elif optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(network.parameters(),\n",
        "                               lr=learning_rate)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def train_epoch(network, loader, optimizer):\n",
        "    cumu_loss = 0\n",
        "    for _, (data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "       # print(\" hello train_epoch\")\n",
        "\n",
        "\n",
        "        # âž¡ Forward pass\n",
        "        loss = nn.MSELoss()\n",
        "        loss =loss(network(data), target)\n",
        "        #cumu_loss += loss\n",
        "        lossRMS= nn.MSELoss()\n",
        "        RmsLossValue=torch.sqrt(lossRMS(torch.flatten(network(data)), torch.flatten(target)))\n",
        "\n",
        "        # â¬… Backward pass + weight update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #wandb.log({\"batch loss\": RmsLossValue, \"RMS Loss\" : RmsLossValue})\n",
        "\n",
        "    return RmsLossValue"
      ],
      "outputs": [],
      "metadata": {
        "id": "4O8IjLcKUxkC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# loader = build_dataset(2)\n",
        "# wandb.init()\n",
        "# config = wandb.config\n",
        "# network = build_network(config)\n",
        "# optimizer = build_optimizer(network, \"sgd\", 0.1)  \n",
        "# cumu_loss = 0\n",
        "\n",
        "\n",
        "# for epoch in range(1):\n",
        "#     for _, (data, target) in enumerate(loader):\n",
        "#             data, target = data.to(device), target.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "#             # âž¡ Forward pass\n",
        "#             loss = nn.MSELoss()\n",
        "#             loss =loss(network(data), target)\n",
        "\n",
        "#             lossRMS= nn.MSELoss()\n",
        "#             RmsLossValue=torch.sqrt(lossRMS(torch.flatten(network(data)), torch.flatten(target)))\n",
        "\n",
        "\n",
        "#             out= network(data)\n",
        "#             #cumu_loss += loss\n",
        "\n",
        "#             # â¬… Backward pass + weight update\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# a = torch.flatten(torch.subtract(network(data),target))\n",
        "# lossRMS= nn.MSELoss()\n",
        "# a=torch.sqrt(lossRMS(torch.flatten(network(data)), torch.flatten(target)))\n"
      ],
      "outputs": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Organizing Hyperparameter Sweeps in PyTorch with W&B",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "643de512da3ef9692daa896915c9c6edbec44805ccf75c0ed51aafb5eb9790df"
    },
    "kernelspec": {
      "display_name": "Python 3.8.2 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}