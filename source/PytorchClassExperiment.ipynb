{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Learn about class and pytorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch ingnite"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install pytorch-ignite"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build data loader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pyarrow.feather as feather\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## generate testdata"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "zernikeData_df_dummy = pd.DataFrame().reindex_like(zernikeData_df)\n",
    "zernikeData_df_dummy[\"zernikeCoeef1\"] =1\n",
    "zernikeData_df_dummy[\"zernikeCoeef2\"] =2\n",
    "zernikeData_df_dummy[\"zernikeCoeef3\"] =3\n",
    "zernikeData_df_dummy[\"zernikeCoeef4\"] =4\n",
    "zernikeData_df_dummy[\"zernikeCoeef5\"] =5\n",
    "zernikeData_df_dummy[\"zernikeCoeef6\"] =6\n",
    "zernikeData_df_dummy[\"zernikeCoeef7\"] =7\n",
    "zernikeData_df_dummy[\"zernikeCoeef8\"] =8\n",
    "zernikeData_df_dummy[\"zernikeCoeef9\"] =9"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    zernikeData_df = feather.read_feather('data/zernikeData.feather')\n",
    "    useDummy = 0\n",
    "    if useDummy == 1:\n",
    "        print(\"fummy\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(fluxData_df, zernikeData_df_dummy, test_size=0.1, random_state=42)\n",
    "    else:\n",
    "        print(\"fummyfasfdsadf\")\n",
    "        X_train, X_val, y_train, y_val = train_test_split(fluxData_df, zernikeData_df, test_size=0.1, random_state=42)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_dataset(batchSizeTrain, batchsizeValid):\n",
    "    fluxData_df = feather.read_feather('data/fluxData.feather')\n",
    "    zernikeData_df = feather.read_feather('data/zernikeData.feather')\n",
    "    X_train, X_val, y_train, y_val = train_test_split(fluxData_df, zernikeData_df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "    train_target = torch.tensor(y_train.values.astype(np.float32))\n",
    "    trainInput = torch.tensor(X_train.values.astype(np.float32))\n",
    "\n",
    "    train_tensor = torch.utils.data.TensorDataset(trainInput, train_target) \n",
    "    loaderTrain = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batchSizeTrain, shuffle = True)\n",
    "\n",
    "\n",
    "    valid_target = torch.tensor(y_val.values.astype(np.float32))\n",
    "    validInput = torch.tensor(X_val.values.astype(np.float32))\n",
    "\n",
    "    train_tensor = torch.utils.data.TensorDataset(validInput, valid_target) \n",
    "    loaderValid = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batchsizeValid, shuffle = True)\n",
    "    return loaderTrain, loaderValid\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loaderTrain, loaderValid =build_dataset(64,1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "next(iter(loaderValid))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "loaderValid.batch_sampler.sampler.num_samples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This cell defines the four pieces of our training procedure:\n",
    "`build_dataset`, `build_network`, `build_optimizer`, and `train_epoch`.\n",
    "\n",
    "All of these are a standard part of a basic PyTorch pipeline,\n",
    "and their implementation is unaffected by the use of W&B,\n",
    "so we won't comment on them."
   ],
   "metadata": {
    "id": "MB2x_e11Wzkq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_dataset(batch_size):\n",
    "    fluxData_df = feather.read_feather('data/fluxData.feather')\n",
    "    zernikeData_df = feather.read_feather('data/zernikeData.feather')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     fluxData_df, zernikeData_df, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "    train_target = torch.tensor(zernikeData_df.values.astype(np.float32))\n",
    "    train = torch.tensor(fluxData_df.values.astype(np.float32))\n",
    "\n",
    "    train_tensor = torch.utils.data.TensorDataset(train, train_target) \n",
    "    loader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "   \n",
    "\n",
    "\n",
    "    return loader\n",
    "\n",
    "def build_network():\n",
    "    network = nn.Sequential( \n",
    "    nn.Linear(19,2000), nn.ReLU(),\n",
    "    nn.Linear(2000,1050), nn.ReLU(),\n",
    "    nn.Linear(1050,100), nn.ReLU(),\n",
    "    nn.Linear(100, 9))\n",
    "\n",
    "    return network.to(device)\n",
    "        \n",
    "\n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_epoch(network, loader, optimizer):\n",
    "    cumu_loss = 0\n",
    "    for _, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "       # print(\" hello train_epoch\")\n",
    "\n",
    "\n",
    "        # ➡ Forward pass\n",
    "        loss = nn.MSELoss()\n",
    "        loss =loss(network(data), target)\n",
    "        #cumu_loss += loss\n",
    "        lossRMS= nn.MSELoss()\n",
    "        RmsLossValue=torch.sqrt(lossRMS(torch.flatten(network(data)), torch.flatten(target)))\n",
    "\n",
    "        # ⬅ Backward pass + weight update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #wandb.log({\"batch loss\": RmsLossValue, \"RMS Loss\" : RmsLossValue})\n",
    "\n",
    "    return RmsLossValue"
   ],
   "outputs": [],
   "metadata": {
    "id": "4O8IjLcKUxkC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build AO network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(19,2000)\n",
    "        self.linear2 = nn.Linear(2000,1050)\n",
    "        self.linear3 = nn.Linear(1050,100)\n",
    "        self.linear3 = nn.Linear(1050,100)\n",
    "        self.out = nn.Linear(100,9)\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU()\n",
    "    })\n",
    "\n",
    "    def forward(self, x, act):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activations[act](x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activations[act](x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activations[act](x)\n",
    "        x = self.out(x)\n",
    "        return(x)\n",
    "\n",
    "    def build_optimizer(self, optimizer, learning_rate):\n",
    "        if optimizer == \"sgd\":\n",
    "            optimizer = optim.SGD(self.network.parameters(),\n",
    "                                lr=learning_rate, momentum=0.9)\n",
    "        elif optimizer == \"adam\":\n",
    "            optimizer = optim.Adam(network.parameters(),\n",
    "                                lr=learning_rate)\n",
    "        return optimizer   \n",
    "    def trainStep(self, loader):\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            inputs, labels = data\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "ClassNetwork = Net()\n",
    "optimizer = optim.SGD(ClassNetwork.parameters(), lr=0.001, momentum=0.9)\n",
    "loaderTrain, loaderValid =build_dataset(64,32)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(loaderValid,0):\n",
    "        input, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ClassNetwork(input, \"relu\")\n",
    "        loss = nn.MSELoss()\n",
    "        loss =loss(ClassNetwork(input, \"relu\"), labels)\n",
    "        RmsLossValid=torch.sqrt(loss)\n",
    "# ⬅ Backward pass + weight update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch: \", epoch, \"loss: \", \"%.4f\" % loss.item() ,\"RmsLossValid: \" , \"%.4f\" % RmsLossValid.item())  \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "torch.sqrt(loss)*5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a=ClassNetwork.build_optimizer(\"sgd\", 1e-3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a.zero_grad()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}