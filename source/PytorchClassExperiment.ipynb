{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Learn about class and pytorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch ingnite"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "!pip install pytorch-ignite"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pytorch-ignite\n",
      "  Downloading pytorch_ignite-0.4.5-py3-none-any.whl (221 kB)\n",
      "\u001b[K     |████████████████████████████████| 221 kB 6.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /home/uwe/anaconda3/lib/python3.8/site-packages (from pytorch-ignite) (1.7.1)\n",
      "Requirement already satisfied: typing_extensions in /home/uwe/anaconda3/lib/python3.8/site-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.2)\n",
      "Requirement already satisfied: numpy in /home/uwe/anaconda3/lib/python3.8/site-packages (from torch<2,>=1.3->pytorch-ignite) (1.19.5)\n",
      "Installing collected packages: pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.4.5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build data loader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import pyarrow.feather as feather\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "fluxData_df = feather.read_feather('data/fluxData.feather')\n",
    "zernikeData_df = feather.read_feather('data/zernikeData.feather')\n",
    "X_train, X_val, y_train, y_val = train_test_split(fluxData_df, zernikeData_df, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "train_target = torch.tensor(y_train.values.astype(np.float32))\n",
    "trainInput = torch.tensor(X_train.values.astype(np.float32))\n",
    "\n",
    "train_tensor = torch.utils.data.TensorDataset(trainInput, train_target) \n",
    "loaderTrain = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = 64, shuffle = True)\n",
    "\n",
    "\n",
    "valid_target = torch.tensor(zernikeData_df.values.astype(np.float32))\n",
    "validInput = torch.tensor(fluxData_df.values.astype(np.float32))\n",
    "\n",
    "train_tensor = torch.utils.data.TensorDataset(validInput, valid_target) \n",
    "loaderValid = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = 3, shuffle = True)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "next(iter(loaderValid))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor([[3.9838, 0.7945, 1.1854, 0.5048, 1.2628, 0.7608, 1.8350, 1.7367, 0.3203,\n",
       "          1.0304, 1.1907, 1.0754, 0.3224, 0.7151, 1.1101, 0.6950, 0.3608, 0.2311,\n",
       "          0.2402],\n",
       "         [3.6214, 1.4549, 1.7598, 1.0048, 1.4599, 0.6538, 0.9810, 1.5470, 0.2775,\n",
       "          2.0602, 0.8395, 0.8128, 0.3921, 0.4869, 1.0023, 0.8948, 0.3750, 0.1900,\n",
       "          0.2975],\n",
       "         [4.8043, 1.1849, 1.0833, 0.8343, 1.1422, 0.8732, 2.6207, 0.6068, 0.2824,\n",
       "          0.7544, 0.9943, 0.6023, 0.7088, 0.3697, 0.3085, 0.4367, 0.2326, 0.3506,\n",
       "          0.2417]]),\n",
       " tensor([[-0.2079, -0.3518,  0.1227,  0.3991,  0.0929,  0.2048, -0.1164, -0.2364,\n",
       "           0.0084],\n",
       "         [-0.2178, -0.3519,  0.1679,  0.4260, -0.0993,  0.1191,  0.1388,  0.0766,\n",
       "           0.0916],\n",
       "         [ 0.0759, -0.1005, -0.1488, -0.0856,  0.2493, -0.2654, -0.1965,  0.2039,\n",
       "           0.2004]])]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "loaderValid.batch_sampler.sampler.num_samples"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "58740"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This cell defines the four pieces of our training procedure:\n",
    "`build_dataset`, `build_network`, `build_optimizer`, and `train_epoch`.\n",
    "\n",
    "All of these are a standard part of a basic PyTorch pipeline,\n",
    "and their implementation is unaffected by the use of W&B,\n",
    "so we won't comment on them."
   ],
   "metadata": {
    "id": "MB2x_e11Wzkq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def build_dataset(batch_size):\n",
    "    fluxData_df = feather.read_feather('data/fluxData.feather')\n",
    "    zernikeData_df = feather.read_feather('data/zernikeData.feather')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "...     fluxData_df, zernikeData_df, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "    train_target = torch.tensor(zernikeData_df.values.astype(np.float32))\n",
    "    train = torch.tensor(fluxData_df.values.astype(np.float32))\n",
    "\n",
    "    train_tensor = torch.utils.data.TensorDataset(train, train_target) \n",
    "    loader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "   \n",
    "\n",
    "\n",
    "    return loader\n",
    "\n",
    "def build_network():\n",
    "    network = nn.Sequential( \n",
    "    nn.Linear(19,2000), nn.ReLU(),\n",
    "    nn.Linear(2000,1050), nn.ReLU(),\n",
    "    nn.Linear(1050,100), nn.ReLU(),\n",
    "    nn.Linear(100, 9))\n",
    "\n",
    "    return network.to(device)\n",
    "        \n",
    "\n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_epoch(network, loader, optimizer):\n",
    "    cumu_loss = 0\n",
    "    for _, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "       # print(\" hello train_epoch\")\n",
    "\n",
    "\n",
    "        # ➡ Forward pass\n",
    "        loss = nn.MSELoss()\n",
    "        loss =loss(network(data), target)\n",
    "        #cumu_loss += loss\n",
    "        lossRMS= nn.MSELoss()\n",
    "        RmsLossValue=torch.sqrt(lossRMS(torch.flatten(network(data)), torch.flatten(target)))\n",
    "\n",
    "        # ⬅ Backward pass + weight update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #wandb.log({\"batch loss\": RmsLossValue, \"RMS Loss\" : RmsLossValue})\n",
    "\n",
    "    return RmsLossValue"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-638d2d89242b>, line 5)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-638d2d89242b>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    ...     fluxData_df, zernikeData_df, test_size=0.33, random_state=42)\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "metadata": {
    "id": "4O8IjLcKUxkC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " hello train\n",
      "0\n",
      " hello poch ende\n",
      "tensor(0.0476, device='cuda:0', grad_fn=<SqrtBackward>)\n",
      " hello train\n",
      "1\n",
      " hello poch ende\n",
      "tensor(0.0427, device='cuda:0', grad_fn=<SqrtBackward>)\n",
      " hello train\n",
      "2\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[0;32m<ipython-input-5-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e28cb217c969>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n",
      "\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" hello train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 19\u001b[0;31m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" hello poch ende\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m<ipython-input-4-bb9490618fb0>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(network, loader, optimizer)\u001b[0m\n",
      "\u001b[1;32m     37\u001b[0m     \u001b[0mcumu_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     41\u001b[0m        \u001b[0;31m# print(\" hello train_epoch\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build AO network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(19,2000)\n",
    "        self.linear2 = nn.Linear(2000,1050)\n",
    "        self.linear3 = nn.Linear(1050,100)\n",
    "        self.linear3 = nn.Linear(1050,100)\n",
    "        self.out = nn.Linear(100,19)\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'lrelu': nn.LeakyReLU()\n",
    "    })\n",
    "\n",
    "    def forward(self, x, act):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activations[act](x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activations[act](x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activations[act](x)\n",
    "        x = self.out(x)\n",
    "        return(x)\n",
    "\n",
    "\n",
    "ClassNetwork = Net()\n",
    "sample_input = torch.randn(19)\n",
    "ClassNetwork(sample_input, \"relu\")\n",
    "#print(ClassNetwork)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.0156, -0.0943,  0.0032,  0.1251, -0.1313,  0.0721, -0.0457,  0.1579,\n",
       "         0.1255,  0.0058,  0.0656, -0.0503, -0.0087, -0.0416,  0.1287, -0.1405,\n",
       "        -0.0949,  0.0222,  0.1019], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}